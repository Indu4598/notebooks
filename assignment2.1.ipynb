{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "### Assignment 2\nWelcome to Assignment 2. This will be fun. It is the first time you actually access external data from ApacheSpark. \n\nJust make sure you hit the play button on each cell from top to down. There are three functions you have to implement. Please also make sure than on each change on a function you hit the play button again on the corresponding cell to make it available to the rest of this notebook.\n\n##### Please also make sure to only implement the function bodies and DON'T add any additional code outside functions since this might confuse the autograder.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "So the function below is used to make it easy for you to create a data frame from a cloudant data frame using the so called \"DataSource\" which is some sort of a plugin which allows ApacheSpark to use different data sources.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Please don't modify this function\ndef readDataFrameFromCloudant(database):\n    cloudantdata=spark.read.load(database, \"org.apache.bahir.cloudant\")\n\n    cloudantdata.createOrReplaceTempView(\"washing\")\n    spark.sql(\"SELECT * from washing\").show()\n    return cloudantdata"
        }, 
        {
            "source": "This is the first function you have to implement. You are passed a dataframe object. We've also registered the dataframe in the ApacheSparkSQL catalog - so you can also issue queries against the \"washing\" table using \"spark.sql()\". Hint: To get an idea about the contents of the catalog you can use: spark.catalog.listTables().\nSo now it's time to implement your first function. You are free to use the dataframe API, SQL or RDD API. In case you want to use the RDD API just obtain the encapsulated RDD using \"df.rdd\". You can test the function by running one of the three last cells of this notebook, but please make sure you run the cells from top to down since some are dependant of each other...", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def count(df,spark):\n    #TODO Please enter your code here\n    return df.count()"
        }, 
        {
            "source": "No it's time to implement the second function. Please return an integer containing the number of fields. The most easy way to get this is using the dataframe API. Hint: You might find the dataframe API documentation useful: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def getNumberOfFields(df,spark):\n    #TODO Please enter your code here\n    return len(df.columns)"
        }, 
        {
            "source": "Finally, please implement a function which returns a (python) list of string values of the field names in this data frame. Hint: Just copy&past doesn't work because the auto-grader will create a random data frame for testing, so please use the data frame API as well. Again, this is the link to the documentation: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def getFieldNames(df,spark):\n    #TODO Please enter your code here\n    return df.columns"
        }, 
        {
            "source": "### PLEASE DON'T REMOVE THIS BLOCK - THE FOLLOWING CODE IS NOT GRADED\n#axx\n### PLEASE DON'T REMOVE THIS BLOCK - THE FOLLOWING CODE IS NOT GRADED\nSo after this block you can basically do what you like since the auto-grader is not considering this part of the notebook", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now it is time to connect to the cloudant database. Please have a look at the Video \"Overview of end-to-end scenario\" of Week 2 starting from 6:40 in order to learn how to obtain the credentials for the database. Please paste this credentials as strings into the below code\n\n### TODO Please provide your Cloudant credentials here", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "hostname = \"https://d19c7e5b-7481-459c-b728-2c5a0cbcbc5c-bluemix:bb7124cb317b18715a5cafcf0c77963ab02b8817d3f00473c430a9e81d9357f1@d19c7e5b-7481-459c-b728-2c5a0cbcbc5c-bluemix.cloudant.com\"\nuser = \"d19c7e5b-7481-459c-b728-2c5a0cbcbc5c-bluemix\"\npw = \"bb7124cb317b18715a5cafcf0c77963ab02b8817d3f00473c430a9e81d9357f1\"\ndatabase = \"washing\" #as long as you didn't change this in the NodeRED flow the database name stays the same"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "Py4JJavaError", 
                    "evalue": "An error occurred while calling o514.load.\n: java.net.UnknownHostException: https\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:214)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403)\n\tat java.net.Socket.connect(Socket.java:666)\n\tat com.ibm.jsse2.av.connect(av.java:933)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:187)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:494)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:589)\n\tat com.ibm.net.ssl.www2.protocol.https.c.<init>(c.java:69)\n\tat com.ibm.net.ssl.www2.protocol.https.c.a(c.java:222)\n\tat com.ibm.net.ssl.www2.protocol.https.d.getNewHttpClient(d.java:28)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1168)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1062)\n\tat com.ibm.net.ssl.www2.protocol.https.d.connect(d.java:12)\n\tat com.ibm.net.ssl.www2.protocol.https.b.connect(b.java:87)\n\tat scalaj.http.DefaultConnectFunc$.apply(Http.scala:471)\n\tat scalaj.http.DefaultConnectFunc$.apply(Http.scala:469)\n\tat scalaj.http.HttpRequest.scalaj$http$HttpRequest$$doConnection(Http.scala:355)\n\tat scalaj.http.HttpRequest.exec(Http.scala:335)\n\tat scalaj.http.HttpRequest.execute(Http.scala:323)\n\tat org.apache.bahir.cloudant.common.JsonStoreDataAccess.getQueryResult(JsonStoreDataAccess.scala:127)\n\tat org.apache.bahir.cloudant.common.JsonStoreDataAccess.getTotalRows(JsonStoreDataAccess.scala:78)\n\tat org.apache.bahir.cloudant.common.JsonStoreRDD.getPartitions(JsonStoreRDD.scala:182)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2004)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1109)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1103)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:69)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:328)\n\tat org.apache.bahir.cloudant.DefaultSource.create(DefaultSource.scala:115)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:94)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:86)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:811)\n", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-23-ca4e00151c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IOT-app-default\"\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.host\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhostname\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.username\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cloudant.password\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpw\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcloudantdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreadDataFrameFromCloudant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m<ipython-input-19-340533c749f9>\u001b[0m in \u001b[0;36mreadDataFrameFromCloudant\u001b[0;34m(database)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Please don't modify this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadDataFrameFromCloudant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcloudantdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.bahir.cloudant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcloudantdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"washing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n", 
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o514.load.\n: java.net.UnknownHostException: https\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:214)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:403)\n\tat java.net.Socket.connect(Socket.java:666)\n\tat com.ibm.jsse2.av.connect(av.java:933)\n\tat sun.net.NetworkClient.doConnect(NetworkClient.java:187)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:494)\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:589)\n\tat com.ibm.net.ssl.www2.protocol.https.c.<init>(c.java:69)\n\tat com.ibm.net.ssl.www2.protocol.https.c.a(c.java:222)\n\tat com.ibm.net.ssl.www2.protocol.https.d.getNewHttpClient(d.java:28)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1168)\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1062)\n\tat com.ibm.net.ssl.www2.protocol.https.d.connect(d.java:12)\n\tat com.ibm.net.ssl.www2.protocol.https.b.connect(b.java:87)\n\tat scalaj.http.DefaultConnectFunc$.apply(Http.scala:471)\n\tat scalaj.http.DefaultConnectFunc$.apply(Http.scala:469)\n\tat scalaj.http.HttpRequest.scalaj$http$HttpRequest$$doConnection(Http.scala:355)\n\tat scalaj.http.HttpRequest.exec(Http.scala:335)\n\tat scalaj.http.HttpRequest.execute(Http.scala:323)\n\tat org.apache.bahir.cloudant.common.JsonStoreDataAccess.getQueryResult(JsonStoreDataAccess.scala:127)\n\tat org.apache.bahir.cloudant.common.JsonStoreDataAccess.getTotalRows(JsonStoreDataAccess.scala:78)\n\tat org.apache.bahir.cloudant.common.JsonStoreRDD.getPartitions(JsonStoreRDD.scala:182)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:255)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:253)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2004)\n\tat org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1109)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:381)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1103)\n\tat org.apache.spark.sql.execution.datasources.json.InferSchema$.infer(InferSchema.scala:69)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat org.apache.spark.sql.DataFrameReader$$anonfun$3.apply(DataFrameReader.scala:329)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:328)\n\tat org.apache.bahir.cloudant.DefaultSource.create(DefaultSource.scala:115)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:94)\n\tat org.apache.bahir.cloudant.DefaultSource.createRelation(DefaultSource.scala:86)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:330)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:135)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)\n\tat java.lang.reflect.Method.invoke(Method.java:508)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:811)\n"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "spark = SparkSession\\\n    .builder\\\n    .appName(\"IOT-app-default\")\\\n    .config(\"cloudant.host\",hostname)\\\n    .config(\"cloudant.username\", user)\\\n    .config(\"cloudant.password\",pw)\\\n    .getOrCreate()\ncloudantdata=readDataFrameFromCloudant(database)"
        }, 
        {
            "source": "The following cell can be used to test your count function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'cloudantdata' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-26-56a7e10a510b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudantdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'cloudantdata' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "count(cloudantdata,spark)"
        }, 
        {
            "source": "The following cell can be used to test your getNumberOfFields function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'cloudantdata' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-27-83205866db58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetNumberOfFields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcloudantdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'cloudantdata' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "getNumberOfFields(cloudantdata,spark)"
        }, 
        {
            "source": "The following cell can be used to test your getFieldNames function", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 55, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['_id',\n '_rev',\n 'count',\n 'flowrate',\n 'fluidlevel',\n 'frequency',\n 'hardness',\n 'speed',\n 'temperature',\n 'ts',\n 'voltage']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "getFieldNames(cloudantdata,spark)"
        }, 
        {
            "source": "Congratulations, you are done. So please export this notebook as python script using the \"File->Download as->Python (.py)\" option in the menue of this notebook. This file should be named \"assignment2.1.py\" and uploaded to the autograder of week2.", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}